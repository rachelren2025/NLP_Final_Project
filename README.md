# NLP Final Project (Python)
New York University – College of Arts and Science
## Course Context
This repository contains the group final project for Natural Language Processing at NYU. Our team focused on applying NLP to the domain of legal informatics, specifically evaluating large language models (LLMs) for legal case analysis and question answering. The work emphasized both standard NLP workflows—preprocessing, feature engineering, model training—and domain-specific challenges where correctness has heightened importance due to real-world legal consequences.

## Summary
Natural Language Processing (NLP) models play a critical role in automating complex text-based tasks, including applications in specialized fields like legal informatics. This paper investigates the use of NLP for legal case analysis utilizing large language models (LLM) and evaluating them for their effectiveness in processing legal texts. Multiple evaluation metrics are used to assess the models' performance in accurately interpreting and answering these questions. In the domain-specific field, Legal Question Answering (LQA), correctness is especially valued as there are greater repercussions of incorrect choices. We ran three models, BERT-double, Legal-BERT, and Custom-legal BERT. Instead of using traditional metrics to measure how well each model performs on legal multiple choice questions and answering, we propose a novel metric that incorporates question difficulty, model confidence, and correctness to calculate a more comprehensive score.

The specifics of how we implemented our tasks are written in our final paper titled, "Evaluating Large Language Models for Legal Multiple Choice Question and Answering"

## Project Highlights
- Domain-specific task — Applied NLP methods to Legal Question Answering (LQA), where accuracy and reliability are critical.
- Model Comparison — Trained and evaluated BERT-Double, Legal-BERT, and a Custom-Legal BERT model.
- Novel Metric — Proposed a new evaluation framework that integrates question difficulty, model confidence, and correctness, providing a more comprehensive performance measure than traditional metrics alone.
- Standard Benchmarks — Evaluated models using accuracy, precision, recall, and F1-score for comparability.
- Research Output — Findings are documented in the paper Evaluating Large Language Models for Legal Multiple Choice Question and Answering.

## Skills Developed
- Applying transformer-based models to specialized, domain-specific NLP problems.
- Designing and implementing custom evaluation metrics to capture nuances beyond standard benchmarks.
- Conducting rigorous model comparison across baseline and fine-tuned LLMs.
- Managing end-to-end NLP pipelines, from text preprocessing to final evaluation.
- Communicating technical findings in a structured research paper format.
