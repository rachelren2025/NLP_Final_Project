{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6Bh1fLG_pw"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnoKKOHh3c7"
      },
      "source": [
        "Install required packages and dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6qyL0mI7Fm"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOvcUR-EmbYq"
      },
      "source": [
        "Install transformers from source (required for tokenizers dependencies):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-YEbY41JHGi"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYsF35bTmd8x"
      },
      "source": [
        "Set environment variable to disable tokenizers parallelism:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVr_z0aMOoDI"
      },
      "source": [
        "%env TOKENIZERS_PARALLELISM=false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNX8_0r3HPjE"
      },
      "source": [
        "## Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wfzp3chOwfK"
      },
      "source": [
        "Use the `model_name_or_path` argument to specify the model for `run_glue.py` and `run_multiple_choice.py` scripts.\n",
        "\n",
        "To use the Legal-BERT/Custom Legal-BERT models, pass the following Hugging Face model repository names to the `model_name_or_path` argument:\n",
        "* Legal-BERT: `zlucia/legalbert` (https://huggingface.co/zlucia/legalbert)\n",
        "* Custom Legal-BERT: `zlucia/custom-legalbert` (https://huggingface.co/zlucia/custom-legalbert)\n",
        "* BERT (double): `zlucia/bert-double` (https://huggingface.co/zlucia/bert-double)\n",
        "\n",
        "OR\n",
        "\n",
        "Alternatively, download the model files from the casehold Google Drive folder, unzip `models.zip`, and place the folder inside the top-level directory of the casehold GitHub repository. Then, pass the model paths to the `model_name_or_path` argument:\n",
        "* Legal-BERT: `models/legalbert`\n",
        "* Custom Legal-BERT: `models/custom-legalbert`\n",
        "* BERT (double): `models/bert-double`\n",
        "\n",
        "The following examples run the scripts on the Legal-BERT model. \n",
        "\n",
        "Switch the `model_name_or_path` argument to run the scripts on the Custom Legal-BERT model or the BERT (double) model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8bs5vK_vzDf"
      },
      "source": [
        "### Overruling (or Terms of Service)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgNl63EkngW2"
      },
      "source": [
        "#### Compute pretrain loss\n",
        "To compute per example/average pretrain loss across the full dataset, run the `run_glue.py` script with the arguments specified in the example.\n",
        "- Pass a file containing the full dataset to `validation_file`.\n",
        "- Pass `ptl=True`. \n",
        "- The script requires a `train_file`, but does not use it when `ptl=True`, so the particular file passed is not important in this case.\n",
        "\n",
        "Running the `run_glue.py` script with `ptl=True` writes per example pretrain loss (order matches order of examples in `validation_file`) to the file `per_ex_pretrain_loss.csv` in `output_dir`. The script also prints the average pretrain loss across `validation_file` examples.\n",
        "\n",
        "\n",
        "*Calculate domain specificity (DS) scores*\n",
        "\n",
        "To calculate the domain specificity (DS) score of a task, take the difference in average pretrain loss on BERT (double) and Legal-BERT $$\\overline{L}_{BERT (double)} - \\overline{L}_{Legal-BERT}$$\n",
        "\n",
        "It is also possible to use the script to calculate the DS score of a specific task example $i$ by taking the difference in the example $i$ pretrain loss on BERT (double) and Legal-BERT $$L^{(i)}_{BERT (double)} - L^{(i)}_{Legal-BERT}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0recmEOfUi"
      },
      "source": [
        "# Download model from Hugging Face model repository\n",
        "!python classification/run_glue.py \\\n",
        "  --model_name_or_path zlucia/legalbert \\\n",
        "  --train_file data/overruling/train.csv \\\n",
        "  --validation_file data/overruling/all.csv \\\n",
        "  --ptl=True \\\n",
        "  --max_seq_length 128 \\\n",
        "  --output_dir logs/overruling/legalbert \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSx3sVRkRRXZ"
      },
      "source": [
        "# Assumes access to model downloaded from Google Drive\n",
        "!python classification/run_glue.py \\\n",
        "  --model_name_or_path models/legalbert \\\n",
        "  --train_file data/overruling/train.csv \\\n",
        "  --validation_file data/overruling/all.csv \\\n",
        "  --ptl=True \\\n",
        "  --max_seq_length 128 \\\n",
        "  --output_dir logs/overruling/legalbert \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kC4I539v0SE"
      },
      "source": [
        "#### Finetune\n",
        "\n",
        "To finetune on the dataset, run the `run_glue.py` script with the arguments specified in the example. The hyperparameters specified are the same as those from the paper.\n",
        "- Pass a file containing the train split to `train_file` and a file containing the split to evaluate/predict on (dev or test split) to `validation_file`.\n",
        "- Pass `do_train` to train on `train_file`, `do_eval` to evaluate on `validation_file`, and `do_predict` to predict on `validation_file`. \n",
        "\n",
        "Running the `run_glue.py` script with `do_train` and `do_eval` trains the specified model on `train_file`, evaluates the trained model on `validation_file`, and writes the trained model/tokenizer files and the evaluation results to the file `eval_results.txt` in `output_dir`. Passing `do_predict` writes the class label predictions on `validation_file` to the file `predictions.csv` in `output_dir`. The script also prints the evaluation results on `validation_file` (evaluation F1, evaluation loss etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OkncZINQ1e1"
      },
      "source": [
        "# Download model from Hugging Face model repository\n",
        "!python classification/run_glue.py \\\n",
        "  --model_name_or_path zlucia/legalbert \\\n",
        "  --train_file data/overruling/train.csv \\\n",
        "  --validation_file data/overruling/dev.csv \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size=16 \\\n",
        "  --learning_rate=1e-5 \\\n",
        "  --num_train_epochs=2.0 \\\n",
        "  --output_dir logs/overruling/legalbert \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f24S79PYMkkU"
      },
      "source": [
        "# Assumes access to model downloaded from Google Drive\n",
        "!python classification/run_glue.py \\\n",
        "  --model_name_or_path models/legalbert \\\n",
        "  --train_file data/overruling/train.csv \\\n",
        "  --validation_file data/overruling/dev.csv \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size=16 \\\n",
        "  --learning_rate=1e-5 \\\n",
        "  --num_train_epochs=2.0 \\\n",
        "  --output_dir logs/overruling/legalbert \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lYeB8RQtEcN"
      },
      "source": [
        "### CaseHOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPR44OVG2agY"
      },
      "source": [
        "#### Compute pretrain loss\n",
        "To compute per example/average pretrain loss across the full dataset, run the `run_multiple_choice.py` script with the arguments specified in the example.\n",
        "- Pass `casehold` as `task_name`.\n",
        "- Pass the path to the data directory to `data_dir`. \n",
        "- Pass `ptl=True`.\n",
        "- By default, when `ptl=True`, the script loads the full dataset from the file named `all.csv` in the data directory.\n",
        "- To change the default file names for the splits, edit `utils_multiple_choice.py`.\n",
        "\n",
        "Running the `run_multiple_choice` script with `ptl=True` writes per example pretrain loss to the file `per_ex_pretrain_loss.csv` (order matches order of examples in loaded dataset) in `output_dir`. The script also prints the average pretrain loss across examples.\n",
        "\n",
        "*Calculate domain specificity (DS) scores*\n",
        "\n",
        "To calculate the domain specificity (DS) score of a task, take the difference in average pretrain loss on BERT (double) and Legal-BERT $$\\overline{L}_{BERT (double)} - \\overline{L}_{Legal-BERT}$$\n",
        "\n",
        "It is also possible to use the script to calculate the DS score of a specific task example $i$ by taking the difference in the example $i$ pretrain loss on BERT (double) and Legal-BERT $$L^{(i)}_{BERT (double)} - L^{(i)}_{Legal-BERT}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pJ9tKHvRWke"
      },
      "source": [
        "# Download model from Hugging Face model repository\n",
        "!python multiple_choice/run_multiple_choice.py \\\n",
        "  --task_name casehold \\\n",
        "  --model_name_or_path zlucia/legalbert \\\n",
        "  --data_dir data/casehold \\\n",
        "  --ptl=True \\\n",
        "  --max_seq_length 128 \\\n",
        "  --output_dir logs/casehold/legalbert \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZazQ06vZplB"
      },
      "source": [
        "# Assumes access to model downloaded from Google Drive\n",
        "!python multiple_choice/run_multiple_choice.py \\\n",
        "  --task_name casehold \\\n",
        "  --model_name_or_path models/legalbert \\\n",
        "  --data_dir data/casehold \\\n",
        "  --ptl=True \\\n",
        "  --max_seq_length 128 \\\n",
        "  --output_dir logs/casehold/legalbert \\\n",
        "  --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhD6SrWi3n7Q"
      },
      "source": [
        "#### Finetune\n",
        "\n",
        "To finetune on the dataset, run the `run_multiple_choice.py` script with the arguments specified in the example. The hyperparameters specified are the same as those from the paper.\n",
        "- Pass `casehold` as `task_name`.\n",
        "- Pass a file containing the train split to `train_file` and a file containing the split to evaluate/predict on (dev or test split) to `validation_file`.\n",
        "- Pass `do_train` to train, `do_eval` to evaluate, and `do_predict` to predict.\n",
        "- By default, `ptl=False`, and the script loads the train split as the train dataset from the file named `train.csv` in the data directory and loads the dev split as the evaluation/prediction dataset from the file named `dev.csv` in the data directory. To load the test split as the evaluation/prediction dataset from the file `test.csv` in the data directory, pass `mode=Split.test`.\n",
        "- To change the default file names for the splits, edit `utils_multiple_choice.py`.\n",
        "\n",
        "Running the `run_multiple_choice.py` script with `do_train` and `do_eval` trains the specified model on the train dataset, evaluates the trained model on evaluation dataset, and writes the trained model/tokenizer files and the evaluation results to the file `eval_results.txt` in `output_dir`. Passing `do_predict` writes the class label predictions on the evaluation dataset to the file `predictions.csv` in `output_dir`. The script also prints the evaluation results on `validation_file` (evaluation macro F1, evaluation loss etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r2GG1GfRsAk"
      },
      "source": [
        "# Download model from Hugging Face model repository\n",
        "!python multiple_choice/run_multiple_choice.py \\\n",
        "  --task_name casehold \\\n",
        "  --model_name_or_path zlucia/legalbert \\\n",
        "  --data_dir data/casehold \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size=16 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --num_train_epochs=3.0 \\\n",
        "  --output_dir logs/casehold/legalbert \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgMeJDugUITd"
      },
      "source": [
        "# Assumes access to model downloaded from Google Drive\n",
        "!python multiple_choice/run_multiple_choice.py \\\n",
        "  --task_name casehold \\\n",
        "  --model_name_or_path models/legalbert \\\n",
        "  --data_dir data/casehold \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size=16 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --num_train_epochs=3.0 \\\n",
        "  --output_dir logs/casehold/legalbert \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz81joy6njkZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}